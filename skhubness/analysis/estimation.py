#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
This file is part of scikit-hubness.
The package is available at https://github.com/VarIr/scikit-hubness/
and distributed under the terms of the BSD-3 license.

(c) 2018-2019, Roman Feldbauer
Austrian Research Institute for Artificial Intelligence (OFAI) and
University of Vienna, Division of Computational Systems Biology (CUBE)
Contact: <roman.feldbauer@univie.ac.at>
"""
import logging
from multiprocessing import cpu_count
import numpy as np
from scipy import stats
from scipy.sparse import csr_matrix
from scipy.sparse.base import issparse
from sklearn.neighbors import NearestNeighbors
from sklearn.utils.validation import check_random_state

__all__ = ['Hubness']
VALID_METRICS = ['euclidean', 'cosine', 'precomputed']


class Hubness(object):
    """ Hubness characteristics of data set.

    Parameters
    ----------
    k : int
        Neighborhood size
    hub_size : float
        Hubs are defined as objects with k-occurrence > hub_size * k.
    metric : string, one of ['euclidean', 'cosine', 'precomputed']
        Metric to use for distance computation. Currently, only
        Euclidean, cosine, and precomputed distances are supported.
    k_neighbors : bool
        Whether to save the k-neighbor lists. Requires O(n_test * k) memory.
    k_occurrence : bool
        Whether to save the k-occurrence. Requires O(n_test) memory.
    random_state : int, RandomState instance or None, optional
        CURRENTLY IGNORED.
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.
    shuffle_equal : bool, optional
        If true and metric='precomputed', shuffle neighbors with identical distances
        to avoid artifact hubness.
        NOTE: This is especially useful for secondary distance measures
        with a finite number of possible values, e.g. SNN or MP empiric.
    n_jobs : int, optional
        CURRENTLY IGNORED.
        Number of processes for parallel computations.
        - `1`: Don't use multiprocessing.
        - `-1`: Use all CPUs
    verbose : int, optional
        Level of output messages

    Attributes
    ----------
    k_skewness_ : float
        Hubness, measured as skewness of k-occurrence histogram [1]_
    k_skewness_truncnom : float
        Hubness, measured as skewness of truncated normal distribution
        fitted with k-occurrence histogram
    atkinson_index_ : float
        Hubness, measured as the Atkinson index of k-occurrence distribution
    gini_index_ : float
        Hubness, measured as the Gini index of k-occurrence distribution
    robinhood_index_ : float
        Hubness, measured as Robin Hood index of k-occurrence distribution [2]_
    antihubs_ : int
        Indices to antihubs
    antihub_occurrence_ : float
        Proportion of antihubs in data set
    hubs_ : int
        Indices to hubs
    hub_occurrence_ : float
        Proportion of k-nearest neighbor slots occupied by hubs
    groupie_ratio_ : float
        Proportion of objects with the largest hub in their neighborhood
    k_occurrence_ : ndarray
        Reverse neighbor count for each object
    k_neighbors_ : ndarray
        Indices to k-nearest neighbors for each object

    References
    ----------
    .. [1] `RadovanoviÄ‡, M.; Nanopoulos, A. & Ivanovic, M.
            Hubs in space: Popular nearest neighbors in high-dimensional data.
            Journal of Machine Learning Research, 2010, 11, 2487-2531`
    .. [2] `Feldbauer, R.; Leodolter, M.; Plant, C. & Flexer, A.
            Fast approximate hubness reduction for large high-dimensional data.
            IEEE International Conference of Big Knowledge (2018).`
    """

    def __init__(self, k: int = 10, hub_size: float = 2., metric='euclidean',
                 k_neighbors: bool = False,
                 k_occurrence: bool = False,
                 verbose: int = 0, n_jobs: int = 1, random_state=None,
                 shuffle_equal: bool = True, **kwargs):
        self.k = k
        self.hub_size = hub_size
        self.metric = metric
        self.store_k_neighbors = k_neighbors
        self.store_k_occurrence = k_occurrence
        self.verbose = verbose
        self.n_jobs = n_jobs
        self.random_state = check_random_state(random_state)
        self.shuffle_equal = shuffle_equal
        self.kwargs = kwargs

        # Attributes that are set upon fitting
        self.k_neighbors_ = None
        self.antihubs_ = None
        self.hubs_ = None
        self.antihub_occurrence_ = None
        self.atkinson_index_ = None
        self.gini_index_ = None
        self.groupie_ratio_ = None
        self.robinhood_index_ = None
        self.hub_occurrence_ = None
        self.k_occurrence_ = None
        self.k_skewness_ = None
        self.k_skewness_truncnorm_ = None

        # Making sure parameters have sensible values
        if k is not None:
            if k < 1:
                raise ValueError(f"Neighborhood size 'k' must "
                                 f"be >= 1, but is {k}.")
        if hub_size <= 0:
            raise ValueError(f"Hub size must be greater than zero.")
        if metric not in VALID_METRICS:
            raise ValueError(f"Unknown metric '{metric}'. "
                             f"Must be one of {VALID_METRICS}.")
        if not isinstance(k_neighbors, bool):
            raise ValueError(f"k_neighbors must be True or False.")
        if not isinstance(k_occurrence, bool):
            raise ValueError(f"k_occurrence must be True or False.")
        if n_jobs == -1:
            self.n_jobs = cpu_count()
        elif n_jobs < -1 or n_jobs == 0:
            raise ValueError(f"Number of parallel processes 'n_jobs' must be "
                             f"a positive integer, or ``-1`` to use all local"
                             f" CPU cores. Was {n_jobs} instead.")
        if verbose < 0:
            raise ValueError(f"Verbosity level 'verbose' must be >= 0, "
                             f"but was {verbose}.")
        return

    def _k_neighbors(self, X: np.ndarray = None, Y: np.ndarray = None) -> np.array:
        """ Return indices of nearest neighbors in Y for each vector in X. """
        nn = NearestNeighbors(n_neighbors=self.k, metric=self.metric)
        nn.fit(Y)
        # if X is None, self distances are ignored
        indices = nn.kneighbors(X, return_distance=False)
        return indices

    def _k_neighbors_precomputed(self, D: np.ndarray, kth: np.ndarray, start: int, end: int) -> np.ndarray:
        """ Return indices of nearest neighbors in precomputed distance matrix.

        Note
        ----
        Parameters kth, start, end are used to ensure that objects are
        not returned as their own nearest neighbors.
        """
        n_test, m_test = D.shape
        indices = np.zeros((n_test, self.k), dtype=np.int32)
        for i in range(n_test):
            if self.verbose > 1 \
                    or self.verbose and (i % 1000 == 0 or i + 1 == n_test):
                logging.info(f"k neighbors (from distances): "
                             f"{i+1}/{n_test}.", flush=True)
            d = D[i, :].copy()
            d[~np.isfinite(d)] = np.inf
            if self.shuffle_equal:
                # Randomize equal values in the distance matrix rows to avoid
                # the problem case if all numbers to sort are the same,
                # which would yield high hubness, even if there is none.
                rp = np.random.permutation(m_test)
                d2 = d[rp]
                d2idx = np.argpartition(d2, kth=kth)
                indices[i, :] = rp[d2idx[start:end]]
            else:
                d_idx = np.argpartition(d, kth=kth)
                indices[i, :] = d_idx[start:end]
        return indices

    def _k_neighbors_precomputed_sparse(self, X: csr_matrix, n_samples: int = None) -> np.ndarray:
        """ Find nearest neighbors in sparse distance matrix.

        Parameters
        ----------
        X : sparse, shape = [n_test, n_indexed]
            Sparse distance matrix. Only non-zero elements
            may be considered neighbors.
        n_samples : int
            Number of sampled indexed objects, e.g.
            in approximate hubness reduction.
            If None, this is inferred from the first row of X.

        Returns
        -------
        k_neighbors : ndarray
            Flattened array of neighbor indices.
        """
        if not issparse(X):
            raise TypeError(f'Matrix X is not sparse')
        X = X.tocsr()
        if n_samples is None:
            n_samples = X.indptr[1] - X.indptr[0]
        n_test, _ = X.shape
        # To allow different number of explicit entries per row,
        # we need to process the matrix row-by-row.
        if np.all(X.indptr[1:] - X.indptr[:-1] == n_samples) and not self.shuffle_equal:
            min_ind = np.argpartition(X.data.reshape(n_test, n_samples),
                                      kth=np.arange(self.k),
                                      axis=1)[:, :self.k]
            k_neighbors = X.indices[min_ind.ravel() + np.repeat(X.indptr[:-1], repeats=self.k)]
        else:
            k_neighbors = np.empty((n_test,), dtype=object)
            if self.shuffle_equal:
                for i in range(n_test):
                    if self.verbose > 1 or self.verbose and (i % 1000 == 0 or i + 1 == n_test):
                        logging.info(f"k neighbors (from sparse distances): {i+1}/{n_test}.", flush=True)
                    x = X.getrow(i)
                    rp = self.random_state.permutation(x.nnz)
                    d2 = x.data[rp]
                    d2idx = np.argpartition(d2, kth=np.arange(self.k))
                    k_neighbors[i] = x.indices[rp[d2idx[:self.k]]]
            else:
                for i in range(n_test):
                    if self.verbose > 1 or self.verbose and (i % 1000 == 0 or i + 1 == n_test):
                        logging.info(f"k neighbors (from sparse distances): {i+1}/{n_test}.", flush=True)
                    x = X.getrow(i)
                    min_ind = np.argpartition(x.data, kth=np.arange(self.k))[:self.k]
                    k_neighbors[i] = x.indices[min_ind]
            k_neighbors = np.concatenate(k_neighbors)
        return k_neighbors

    @staticmethod
    def skewness_truncnorm(k_occurrence: np.ndarray) -> float:
        """ Hubness measure; corrected for non-negativity of k-occurrence.

        Hubness as skewness of truncated normal distribution
        estimated from k-occurrence histogram.

        Parameters
        ----------
        k_occurrence : ndarray
            Reverse nearest neighbor count for each object.
        """
        clip_left = 0
        clip_right = np.iinfo(np.int64).max
        k_occurrence_mean = k_occurrence.mean()
        k_occurrence_std = k_occurrence.std(ddof=1)
        a = (clip_left - k_occurrence_mean) / k_occurrence_std
        b = (clip_right - k_occurrence_mean) / k_occurrence_std
        skew_truncnorm = stats.truncnorm(a, b).moment(3)
        return skew_truncnorm

    @staticmethod
    def gini_index(k_occurrence: np.ndarray, limiting='memory') -> float:
        """ Hubness measure; Gini index

        Parameters
        ----------
        k_occurrence : ndarray
            Reverse nearest neighbor count for each object.
        limiting : 'memory' or 'cpu'
            If 'cpu', use fast implementation with high memory usage,
            if 'memory', use slighly slower, but memory-efficient implementation,
            otherwise use naive implementation (slow, low memory usage)
        """
        n = k_occurrence.size
        if limiting.lower() in ['memory', 'space']:
            numerator = np.int(0)
            for i in range(n):
                numerator += np.sum(np.abs(k_occurrence[:] - k_occurrence[i]))
        elif limiting.lower() in ['time', 'cpu']:
            numerator = np.sum(np.abs(k_occurrence.reshape(1, -1) - k_occurrence.reshape(-1, 1)))
        else:  # slow naive implementation
            n = k_occurrence.size
            numerator = 0
            for i in range(n):
                for j in range(n):
                    numerator += np.abs(k_occurrence[i] - k_occurrence[j])
        denominator = 2 * n * np.sum(k_occurrence)
        return numerator / denominator

    @staticmethod
    def robinhood_index(k_occurrence: np.ndarray) -> float:
        """ Hubness measure; Robin hood/Hoover/Schutz index.

        Parameters
        ----------
        k_occurrence : ndarray
            Reverse nearest neighbor count for each object.

        Notes
        -----
        The Robin Hood index was proposed in [1]_ and is especially suited
        for hubness estimation in large data sets. Additionally, it offers
        straight-forward interpretability by answering the question:
        What share of k-occurrence must be redistributed, so that all objects
        are equally often nearest neighbors to others?

        References
        ----------
        .. [1] `Feldbauer, R.; Leodolter, M.; Plant, C. & Flexer, A.
                Fast approximate hubness reduction for large high-dimensional data.
                IEEE International Conference of Big Knowledge (2018).`
        """
        numerator = .5 * float(np.sum(np.abs(k_occurrence - k_occurrence.mean())))
        denominator = float(np.sum(k_occurrence))
        return numerator / denominator

    @staticmethod
    def atkinson_index(k_occurrence: np.ndarray, eps: float = .5) -> float:
        """ Hubness measure; Atkinson index.

        Parameters
        ----------
        k_occurrence : ndarray
            Reverse nearest neighbor count for each object.
        eps : float, default = 0.5
            'Income' weight. Turns the index into a normative measure.
        """
        if eps == 1:
            term = np.prod(k_occurrence) ** (1. / k_occurrence.size)
        else:
            term = np.mean(k_occurrence ** (1 - eps)) ** (1 / (1 - eps))
        return 1. - 1. / k_occurrence.mean() * term

    @staticmethod
    def antihub_occurrence(k_occurrence: np.ndarray) -> (np.array, float):
        """Proportion of antihubs in data set.

        Antihubs are objects that are never among the nearest neighbors
        of other objects.

        Parameters
        ----------
        k_occurrence : ndarray
            Reverse nearest neighbor count for each object.
        """
        antihubs = np.argwhere(k_occurrence == 0).ravel()
        antihub_occurrence = antihubs.size / k_occurrence.size
        return antihubs, antihub_occurrence

    @staticmethod
    def hub_occurrence(k: int, k_occurrence: np.ndarray, n_test: int, hub_size: float = 2):
        """Proportion of nearest neighbor slots occupied by hubs.

        Parameters
        ----------
        k : int
            Specifies the number of nearest neighbors
        k_occurrence : ndarray
            Reverse nearest neighbor count for each object.
        n_test : int
            Number of queries (or objects in a test set)
        hub_size : float
            Factor to determine hubs
        """
        hubs = np.argwhere(k_occurrence >= hub_size * k).ravel()
        hub_occurrence = k_occurrence[hubs].sum() / k / n_test
        return hubs, hub_occurrence

    def estimate(self, X: np.ndarray, Y: np.ndarray = None, has_self_distances: bool = False):
        """ Estimate hubness in a data set.

        Hubness is estimated from the distances between all objects in X to all objects in Y.
        If Y is None, all-against-all distances between the objects in X are used.
        If self.metric == 'precomputed', X must be a distance matrix.

        Parameters
        ----------
        X : ndarray, shape (n_query, n_features) or (n_query, n_indexed)
            Array of query vectors, or distance, if self.metric == 'precomputed'
        Y : ndarray, shape (n_indexed, n_features) or None
            Array of indexed vectors. If None, calculate distance between all pairs of
            objects in X.
        has_self_distances : bool, default = False
            Define, whether a precomputed distance matrix contains self distances,
            which need to be excluded.

        Returns
        -------
        self : Hubness
            An instance of class Hubness is returned. Hubness indices are
            provided as attributes (e.g. :func:`robinhood_index_`).
        """
        return self.fit_transform(X, Y, has_self_distances)

    def fit_transform(self, X, Y=None, has_self_distances=False):
        # Let's assume there are no self distances in X
        kth = np.arange(self.k)
        start = 0
        end = self.k
        if self.metric == 'precomputed':
            if Y is not None:
                raise ValueError(
                    f"Y must be None when using precomputed distances.")
            n_test, n_train = X.shape
            if n_test == n_train and has_self_distances:
                kth = np.arange(self.k + 1)
                start = 1
                end = self.k + 1
        else:
            n_test, m_test = X.shape
            if Y is None:
                # Self distances do occur in this case
                kth = np.arange(self.k + 1)
                start = 1
                end = self.k + 1
                n_train, m_train = X.shape
            else:
                n_train, m_train = Y.shape
            assert m_test == m_train, f'Number of features do not match'

        if self.metric == 'precomputed':
            if issparse(X):
                k_neighbors = self._k_neighbors_precomputed_sparse(X)
            else:
                k_neighbors = self._k_neighbors_precomputed(X, kth, start, end)
        else:
            if Y is None:
                k_neighbors = self._k_neighbors(Y=X)
            else:
                k_neighbors = self._k_neighbors(X=X, Y=Y)
        if self.store_k_neighbors:
            self.k_neighbors_ = k_neighbors
        k_occurrence = np.bincount(
            k_neighbors.astype(int).ravel(), minlength=n_train)
        if self.store_k_occurrence:
            self.k_occurrence_ = k_occurrence
        # traditional skewness measure
        self.k_skewness_ = stats.skew(k_occurrence)
        # new skewness measure (truncated normal distribution)
        self.k_skewness_truncnorm_ = self.skewness_truncnorm(k_occurrence)
        # Gini index
        if k_occurrence.shape[0] > 10_000:
            limiting = 'space'
        else:
            limiting = 'time'
        self.gini_index_ = self.gini_index(k_occurrence, limiting)
        # Robin Hood index
        self.robinhood_index_ = self.robinhood_index(k_occurrence)
        # Atkinson index
        self.atkinson_index_ = self.atkinson_index(k_occurrence)
        # anti-hub occurrence
        self.antihubs_, self.antihub_occurrence_ = \
            self.antihub_occurrence(k_occurrence)
        # hub occurrence
        self.hubs_, self.hub_occurrence_ = \
            self.hub_occurrence(k=self.k, k_occurrence=k_occurrence,
                                n_test=n_test, hub_size=self.hub_size)
        # Largest hub
        self.groupie_ratio_ = k_occurrence.max() / n_test / self.k

        return self
