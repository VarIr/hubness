% Encoding: UTF-8

@Article{Angiulli2018,
  author        = {Fabrizio Angiulli},
  title         = {On the Behavior of Intrinsically High-Dimensional Spaces: Distances, Direct and Reverse Nearest Neighbors, and Hubness},
  journal       = {J. Mach. Learn. Res.},
  year          = {2018},
  volume        = {18},
  pages         = {170:1--170:60},
}

@InProceedings{Flexer2013,
  author      = {Arthur {Flexer} and Dominik {Schnitzer}},
  title       = {Can Shared Nearest Neighbors Reduce Hubness in High-Dimensional Spaces?},
  booktitle   = {Proc. IEEE 13th Int. Conf. Data Mining Workshops},
  year        = {2013},
  pages       = {460--467},
  month       = dec,
  commentdoi  = {10.1109/ICDMW.2013.101},
  commentissn = {2375-9232},
  keywords    = {data handling, learning (artificial intelligence), pattern classification, shared nearest neighbors, machine learning, high dimensional data spaces, data points, SNN, image recognition, real world data sets, Accuracy, Standards, Image recognition, Histograms, Conferences, Electronic mail, Context, machine learning, curse of dimensionality, hubness, shared nearest neighors},
}

@Article{Flexer2015,
  author    = {Flexer, Arthur and Schnitzer, Dominik},
  title     = {Choosing $\ell$p norms in high-dimensional spaces based on hub analysis},
  journal   = {Neurocomputing},
  year      = {2015},
  volume    = {169},
  pages     = {281--287},
  publisher = {Elsevier},
}

@Article{Schnitzer2012,
  author           = {Schnitzer, Dominik and Flexer, Arthur and Schedl, Markus and Widmer, Gerhard},
  title            = {Local and Global Scaling Reduce Hubs in Space},
  journal          = {J. Mach. Learn. Res.},
  year             = {2012},
  volume           = {13},
  number           = {1},
  pages            = {2871--2902},
  month            = oct,
  commentacmid     = {2503333},
  commentissn      = {1532-4435},
  commentpublisher = {JMLR.org},
  commenturl       = {http://dl.acm.org/citation.cfm?id=2503308.2503333},
  issue_date       = {January 2012},
  keywords         = {classification, curse of dimensionality, hubness, local and global scaling, nearest neighbor relation, shared near neighbors},
  numpages         = {32},
}

@Article{Radovanovic2010,
  author           = {Radovanovi\'{c}, Milo\v{s} and Nanopoulos, Alexandros and Ivanovi\'{c}, Mirjana},
  title            = {Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data},
  journal          = {J. Mach. Learn. Res.},
  year             = {2010},
  volume           = {11},
  pages            = {2487--2531},
  month            = dec,
  commentacmid     = {1953015},
  commentissn      = {1532-4435},
  commentpublisher = {JMLR.org},
  commenturl       = {http://dl.acm.org/citation.cfm?id=1756006.1953015},
  issue_date       = {3/1/2010},
  numpages         = {45},
}

@InProceedings{Suzuki2013,
  author           = {Suzuki, Ikumi and Hara, Kazuo and Shimbo, Masashi and Saerens, Marco and Fukumizu, Kenji},
  title            = {Centering Similarity Measures to Reduce Hubs},
  booktitle        = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  year             = {2013},
  pages            = {613--623},
  address          = {Seattle, Washington, USA},
  month            = oct,
  commentpublisher = {Association for Computational Linguistics},
  commenturl       = {https://www.aclweb.org/anthology/D13-1058},
}

@Article{Feldbauer2019,
  author      = {Feldbauer, Roman and Flexer, Arthur},
  title       = {A comprehensive empirical comparison of hubness reduction in high-dimensional spaces},
  journal     = {Knowledge and Information Systems},
  year        = {2019},
  volume      = {59},
  number      = {1},
  pages       = {137},
  month       = apr,
  abstract    = {Hubness is an aspect of the curse of dimensionality related to the distance concentration effect. Hubs occur in high-dimensional data spaces as objects that are particularly often among the nearest neighbors of other objects. Conversely, other data objects become antihubs, which are rarely or never nearest neighbors to other objects. Many machine learning algorithms rely on nearest neighbor search and some form of measuring distances, which are both impaired by high hubness. Degraded performance due to hubness has been reported for various tasks such as classification, clustering, regression, visualization, recommendation, retrieval and outlier detection. Several hubness reduction methods based on different paradigms have previously been developed. Local and global scaling as well as shared neighbors approaches aim at repairing asymmetric neighborhood relations. Global and localized centering try to eliminate spatial centrality, while the related global and local dissimilarity measures are based on density gradient flattening. Additional methods and alternative dissimilarity measures that were argued to mitigate detrimental effects of distance concentration also influence the related hubness phenomenon. In this paper, we present a large-scale empirical evaluation of all available unsupervised hubness reduction methods and dissimilarity measures. We investigate several aspects of hubness reduction as well as its influence on data semantics which we measure via nearest neighbor classification. Scaling and density gradient flattening methods improve evaluation measures such as hubness and classification accuracy consistently for data sets from a wide range of domains, while centering approaches achieve the same only under specific settings.},
  commentdoi  = {10.1007/s10115-018-1205-y},
  commentissn = {0219-3116},
  commenturl  = {http://dx.doi.org/10.1007/s10115-018-1205-y},
  date        = {2019-04-04},
  publisher   = {Springer},
}

@InProceedings{Feldbauer2018,
  author     = {Roman {Feldbauer} and Maximilian {Leodolter} and Claudia {Plant} and Arthur {Flexer}},
  title      = {Fast Approximate Hubness Reduction for Large High-Dimensional Data},
  booktitle  = {Proc. IEEE Int. Conf. Big Knowledge (ICBK)},
  year       = {2018},
  pages      = {358--367},
  month      = nov,
  commentdoi = {10.1109/ICBK.2018.00055},
  keywords   = {computational complexity, data analysis, data mining, mobile computing, public domain software, software packages, mobile device, open source software package, high-dimensional data mining, fast approximate hubness reduction, massive mobility data, linear complexity, quadratic algorithmic complexity, dimensionality curse, Complexity theory, Indexes, Estimation, Data mining, Approximation algorithms, Time measurement, curse of dimensionality, high-dimensional data mining, hubness, linear complexity, interpretability, smartphones, transport mode detection},
}

@InProceedings{Hara2015,
  author     = {Kazuo Hara and Ikumi Suzuki and Masashi Shimbo and Kei Kobayashi and Kenji Fukumizu and Miloš Radovanović},
  title      = {Localized Centering: Reducing Hubness in Large-Sample Data},
  booktitle  = {AAAI Conference on Artificial Intelligence},
  year       = {2015},
  abstract   = {Hubness has been recently identified as a problematic phenomenon occurring in high-dimensional space. In this paper, we address a different type of hubness that occurs when the number of samples is large. We investigate the difference between the hubness in high-dimensional data and the one in large-sample data. One finding is that centering, which is known to reduce the former, does not work for the latter. We then propose a new hub-reduction method, called localized centering. It is an extension of centering, yet works effectively for both types of hubness. Using real-world datasets consisting of a large number of documents, we demonstrate that the proposed method improves the accuracy of k-nearest neighbor classification.},
  commenturl = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9898},
  conference = {AAAI Conference on Artificial Intelligence},
  keywords   = {Hubness; Centering; k nearest neighbor method},
}

@InProceedings{Hara2016,
  author     = {Kazuo Hara and Ikumi Suzuki and Kei Kobayashi and Kenji Fukumizu and Milos Radovanovic},
  title      = {Flattening the Density Gradient for Eliminating Spatial Centrality to Reduce Hubness},
  booktitle  = {AAAI Conference on Artificial Intelligence},
  year       = {2016},
  abstract   = {Spatial centrality, whereby samples closer to the center of a dataset tend to be closer to all other samples, is regarded as one source of hubness. Hubness is well known to degrade k-nearest-neighbor (k-NN) classification. Spatial centrality can be removed by centering, i.e., shifting the origin to the global center of the dataset, in cases where inner product similarity is used. However, when Euclidean distance is used, centering has no effect on spatial centrality because the distance between the samples is the same before and after centering. As described in this paper, we propose a solution for the hubness problem when Euclidean distance is considered. We provide a theoretical explanation to demonstrate how the solution eliminates spatial centrality and reduces hubness. We then present some discussion of the reason the proposed solution works, from a viewpoint of density gradient, which is regarded as the origin of spatial centrality and hubness. We demonstrate that the solution corresponds to flattening the density gradient. Using real-world datasets, we demonstrate that the proposed method improves k-NN classification performance and outperforms an existing hub-reduction method.},
  commenturl = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12055},
  conference = {AAAI Conference on Artificial Intelligence},
  keywords   = {Hubness; Density gradient; Spatial centrality; k nearest neighbor method},
}

@Article{Virtanen2019,
  author      = {Pauli Virtanen and Ralf Gommers and Travis E. Oliphant and Matt Haberland and others},
  title       = {{SciPy} 1.0--Fundamental Algorithms for Scientific Computing in {Python}},
  journal     = {arXiv preprint},
  year        = {2019},
  abstract    = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  date        = {2019-07-23},
  eprint      = {http://arxiv.org/abs/1907.10121v1},
  eprintclass = {cs.MS},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1907.10121v1:PDF},
  keywords    = {cs.MS, cs.DS, cs.SE, physics.comp-ph},
  url         = {http://arxiv.org/abs/1907.10121v1},
}

@Article{Pedregosa2011,
  author       = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and others},
  title        = {Scikit-learn: Machine Learning in {Python}},
  journal      = {J. Mach. Learn. Res.},
  year         = {2011},
  volume       = {12},
  pages        = {2825--2830},
  month        = nov,
  commentacmid = {2078195},
  commentissn  = {1532-4435},
  commenturl   = {http://dl.acm.org/citation.cfm?id=1953048.2078195},
  issue_date   = {2/1/2011},
  numpages     = {6},
  publisher    = {JMLR.org},
}

@InCollection{Andoni2015,
  author        = {Andoni, Alexandr and Indyk, Piotr and Laarhoven, Thijs and Razenshteyn, Ilya and Schmidt, Ludwig},
  title         = {Practical and Optimal {LSH} for Angular Distance},
  booktitle     = {Advances in Neural Information Processing Systems 28},
  publisher     = {Curran Associates, Inc.},
  year          = {2015},
  pages         = {1225--1233},
  commenteditor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  commenturl    = {http://papers.nips.cc/paper/5893-practical-and-optimal-lsh-for-angular-distance.pdf},
}

@Article{Malkov16,
  author        = {Yury A. Malkov and D. A. Yashunin},
  title         = {Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs},
  journal       = {arXiv preprint},
  year          = {2016},
  commentvolume = {abs/1603.09320},
  url           = {https://arxiv.org/abs/1603.09320},
}

@Article{Tomasev2014,
  author   = {N. {Tomasev} and M. {Radovanovic} and D. {Mladenic} and M. {Ivanovic}},
  title    = {The Role of Hubness in Clustering High-Dimensional Data},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  year     = {2014},
  volume   = {26},
  number   = {3},
  pages    = {739--751},
  month    = mar,
  issn     = {1041-4347},
  doi      = {10.1109/TKDE.2013.25},
  keywords = {data mining, pattern clustering, hubness role, high-dimensional data clustering, data mining techniques, data sparsity, lower dimensional feature subspace, high-dimensional phenomena, point centrality, high-dimensional data cluster, hubness-based clustering algorithm, cluster prototypes, centroid-based cluster configuration, hyperspherical cluster detection, Correlation, Clustering algorithms, Prototypes, Approximation algorithms, Gaussian distribution, Partitioning algorithms, Educational institutions, Clustering, curse of dimensionality, nearest neighbors, hubs},
}

@InProceedings{Schnitzer2015,
  author       = {Dominik Schnitzer and Arthur Flexer},
  title        = {The unbalancing effect of hubs on K-medoids clustering in high-dimensional spaces},
  booktitle    = {Proc. Int. Joint Conf. Neural Networks (IJCNN)},
  year         = {2015},
  month        = jul,
  commentdoi   = {10.1109/IJCNN.2015.7280303},
  commentpages = {1--8},
  issn         = {2161-4407},
  keywords     = {learning (artificial intelligence), pattern clustering, set theory, unbalancing effect, K-medoids clustering, high-dimensional space, unbalanced cluster solution, hubness, machine learning, high dimensional data space, data point, anti-hubs, unbalanced cluster size, internal evaluation index, external evaluation index, clusters evaluation index, artificial data set, real data set, Tin, Biology},
}

@Article{Buza2015,
  author    = {Krisztian Buza and Alexandros Nanopoulos and G{\'{a}}bor I. Nagy},
  title     = {Nearest neighbor regression in the presence of bad hubs},
  journal   = {Knowl.-Based Syst.},
  year      = {2015},
  volume    = {86},
  pages     = {250--260},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/journals/kbs/BuzaNN15},
  doi       = {10.1016/j.knosys.2015.06.010},
}

@InProceedings{Flexer2015a,
  author        = {Arthur Flexer},
  title         = {Improving Visualization of High-Dimensional Music Similarity Spaces},
  booktitle     = {Proceedings of the 16th International Society for Music Information Retrieval Conference, {ISMIR} 2015},
  year          = {2015},
  pages         = {547--553},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  commentbiburl = {https://dblp.org/rec/bib/conf/ismir/Flexer15},
  commenteditor = {Meinard M{\"{u}}ller and Frans Wiering},
  commenturl    = {http://ismir2015.uma.es/articles/35\_Paper.pdf},
}

@InProceedings{Aumueller2019,
  author           = {Martin Aum{\"{u}}ller and Tobias Christiani and Rasmus Pagh and Michael Vesterli},
  title            = {{PUFFINN:} Parameterless and Universally Fast FInding of Nearest Neighbors},
  booktitle        = {27th Annual European Symposium on Algorithms, {ESA}},
  year             = {2019},
  volume           = {144},
  pages            = {10:1--10:16},
  __markedentry    = {[feldbauer:]},
  commentbibsource = {dblp computer science bibliography, https://dblp.org},
  commentbiburl    = {https://dblp.org/rec/bib/conf/esa/0001CPV19},
  commentdoi       = {10.4230/LIPIcs.ESA.2019.10},
  commenteditor    = {Michael A. Bender and Ola Svensson and Grzegorz Herman},
  commentpublisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
  commentseries    = {LIPIcs},
}

@Article{Angiulli2017,
  author        = {Fabrizio Angiulli},
  title         = {On the Behavior of Intrinsically High-Dimensional Spaces: Distances, Direct and Reverse Nearest Neighbors, and Hubness},
  journal       = {J. Mach. Learn. Res.},
  year          = {2017},
  volume        = {18},
  pages         = {170:1--170:60},
  __markedentry = {[feldbauer:6]},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/jmlr/Angiulli17},
  url           = {http://jmlr.org/papers/v18/17-151.html},
}

@Comment{jabref-meta: databaseType:bibtex;}
